{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "achitectures with changes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUlL4XXgMK0f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WviSDmDQ0mZ7"
      },
      "source": [
        "#3D Unet model:\n",
        "simpleUNet = False\n",
        "\n",
        "class UNetwork():\n",
        "    \n",
        "    def conv_batch_relu(self, tensor, filters, kernel = [3,3,3], stride = [1,1,1], is_training = True):\n",
        "        # Produces the conv_batch_relu combination as in the paper\n",
        "        padding = 'valid'\n",
        "        if self.should_pad: padding = 'same'\n",
        "    \n",
        "        conv = tf.layers.conv3d(tensor, filters, kernel_size = kernel, strides = stride, padding = padding,\n",
        "                                kernel_initializer = self.base_init, kernel_regularizer = self.reg_init)\n",
        "        conv = tf.layers.batch_normalization(conv, training = is_training)\n",
        "        conv = tf.nn.relu(conv) \n",
        "        return conv\n",
        "\n",
        "    def upconvolve(self, tensor, filters, kernel = 2, stride = 2, scale = 4, activation = None):\n",
        "        # Upconvolution - two different implementations: the first is as suggested in the original Unet paper and the second is a more recent version\n",
        "        # Needs to be determined if these do the same thing\n",
        "        padding = 'valid'\n",
        "        if self.should_pad: padding = 'same'\n",
        "        # upsample_routine = tf.keras.layers.UpSampling3D(size = (scale,scale,scale)) # Uses tf.resize_images\n",
        "        # tensor = upsample_routine(tensor)\n",
        "        # conv = tf.layers.conv3d(tensor, filters, kernel, stride, padding = 'same',\n",
        "        #                                 kernel_initializer = self.base_init, kernel_regularizer = self.reg_init)\n",
        "        # use_bias = False is a tensorflow bug\n",
        "        conv = tf.layers.conv3d_transpose(tensor, filters, kernel_size = kernel, strides = stride, padding = padding, use_bias=False, \n",
        "                                          kernel_initializer = self.base_init,  kernel_regularizer = self.reg_init)\n",
        "        return conv\n",
        "\n",
        "    def centre_crop_and_concat(self, prev_conv, up_conv):\n",
        "        # If concatenating two different sized Tensors, centre crop the first Tensor to the right size and concat\n",
        "        # Needed if you don't have padding\n",
        "        p_c_s = prev_conv.get_shape()\n",
        "        u_c_s = up_conv.get_shape()\n",
        "        offsets =  np.array([0, (p_c_s[1] - u_c_s[1]) // 2, (p_c_s[2] - u_c_s[2]) // 2, \n",
        "                             (p_c_s[3] - u_c_s[3]) // 2, 0], dtype = np.int32)\n",
        "        size = np.array([-1, u_c_s[1], u_c_s[2], u_c_s[3], p_c_s[4]], np.int32)\n",
        "        prev_conv_crop = tf.slice(prev_conv, offsets, size)\n",
        "        up_concat = tf.concat((prev_conv_crop, up_conv), 4)\n",
        "        return up_concat\n",
        "        \n",
        "    def __init__(self, base_filt = 8, in_depth = INPUT_DEPTH, out_depth = OUTPUT_DEPTH,\n",
        "                 in_size = INPUT_SIZE, out_size = OUTPUT_SIZE, num_classes = OUTPUT_CLASSES,\n",
        "                 learning_rate = 0.001, print_shapes = True, drop = 0.2, should_pad = False):\n",
        "        # Initialise your model with the parameters defined above\n",
        "        # Print-shape is a debug shape printer for convenience\n",
        "        # Should_pad controls whether the model has padding or not\n",
        "        # Base_filt controls the number of base conv filters the model has. Note deeper analysis paths have filters that are scaled by this value\n",
        "        # Drop specifies the proportion of dropped activations\n",
        "        \n",
        "        self.base_init = tf.truncated_normal_initializer(stddev=0.1) # Initialise weights\n",
        "        self.reg_init = tf.contrib.layers.l2_regularizer(scale=0.1) # Initialise regularisation (was useful)\n",
        "        \n",
        "        self.should_pad = should_pad # To pad or not to pad, that is the question\n",
        "        self.drop = drop # Set dropout rate\n",
        "        \n",
        "        with tf.variable_scope('3DuNet'):\n",
        "            self.training = tf.placeholder(tf.bool)\n",
        "            self.do_print = print_shapes\n",
        "            self.model_input = tf.placeholder(tf.float32, shape = (None, in_depth, in_size, in_size, 1))  \n",
        "            # Define placeholders for feed_dict\n",
        "            self.model_labels = tf.placeholder(tf.int32, shape = (None, out_depth, out_size, out_size, 1))\n",
        "            labels_one_hot = tf.squeeze(tf.one_hot(self.model_labels, num_classes, axis = -1), axis = -2)\n",
        "            \n",
        "            if self.do_print: \n",
        "                print('Input features shape', self.model_input.get_shape())\n",
        "                print('Labels shape', labels_one_hot.get_shape())\n",
        "                \n",
        "            # Level zero\n",
        "            conv_0_1 = self.conv_batch_relu(self.model_input, base_filt, is_training = self.training)\n",
        "            conv_0_2 = self.conv_batch_relu(conv_0_1, base_filt*2, is_training = self.training)\n",
        "            # Level one\n",
        "            max_1_1 = tf.layers.max_pooling3d(conv_0_2, [1,2,2], [1,2,2]) # Stride, Kernel previously [2,2,2]\n",
        "            conv_1_1 = self.conv_batch_relu(max_1_1, base_filt*2, is_training = self.training)\n",
        "            conv_1_2 = self.conv_batch_relu(conv_1_1, base_filt*4, is_training = self.training)\n",
        "            conv_1_2 = tf.layers.dropout(conv_1_2, rate = self.drop, training = self.training)\n",
        "            # Level two\n",
        "            max_2_1 = tf.layers.max_pooling3d(conv_1_2, [1,2,2], [1,2,2]) # Stride, Kernel previously [2,2,2]\n",
        "            conv_2_1 = self.conv_batch_relu(max_2_1, base_filt*4, is_training = self.training)\n",
        "            conv_2_2 = self.conv_batch_relu(conv_2_1, base_filt*8, is_training = self.training)\n",
        "            conv_2_2 = tf.layers.dropout(conv_2_2, rate = self.drop, training = self.training)\n",
        "            \n",
        "            if simpleUNet:\n",
        "                # Level one\n",
        "                up_conv_2_1 = self.upconvolve(conv_2_2, base_filt*8, kernel = 2, stride = [1,2,2]) # Stride previously [2,2,2]\n",
        "            else:\n",
        "                # Level three\n",
        "                max_3_1 = tf.layers.max_pooling3d(conv_2_2, [1,2,2], [1,2,2]) # Stride, Kernel previously [2,2,2]\n",
        "                conv_3_1 = self.conv_batch_relu(max_3_1, base_filt*8, is_training = self.training)\n",
        "                conv_3_2 = self.conv_batch_relu(conv_3_1, base_filt*16, is_training = self.training)\n",
        "                conv_3_2 = tf.layers.dropout(conv_3_2, rate = self.drop, training = self.training)\n",
        "                # Level two\n",
        "                up_conv_3_2 = self.upconvolve(conv_3_2, base_filt*16, kernel = 2, stride = [1,2,2]) # Stride previously [2,2,2] \n",
        "                concat_2_1 = self.centre_crop_and_concat(conv_2_2, up_conv_3_2)\n",
        "                conv_2_3 = self.conv_batch_relu(concat_2_1, base_filt*8, is_training = self.training)\n",
        "                conv_2_4 = self.conv_batch_relu(conv_2_3, base_filt*8, is_training = self.training)\n",
        "                conv_2_4 = tf.layers.dropout(conv_2_4, rate = self.drop, training = self.training)\n",
        "                # Level one\n",
        "                up_conv_2_1 = self.upconvolve(conv_2_4, base_filt*8, kernel = 2, stride = [1,2,2]) # Stride previously [2,2,2]\n",
        "            \n",
        "            concat_1_1 = self.centre_crop_and_concat(conv_1_2, up_conv_2_1)\n",
        "            conv_1_3 = self.conv_batch_relu(concat_1_1, base_filt*4, is_training = self.training)\n",
        "            conv_1_4 = self.conv_batch_relu(conv_1_3, base_filt*4, is_training = self.training)\n",
        "            conv_1_4 = tf.layers.dropout(conv_1_4, rate = self.drop, training = self.training)\n",
        "            # Level zero\n",
        "            up_conv_1_0 = self.upconvolve(conv_1_4, base_filt*4, kernel = 2, stride = [1,2,2])  # Stride previously [2,2,2]\n",
        "            concat_0_1 = self.centre_crop_and_concat(conv_0_2, up_conv_1_0)\n",
        "            conv_0_3 = self.conv_batch_relu(concat_0_1, base_filt*2, is_training = self.training)\n",
        "            conv_0_4 = self.conv_batch_relu(conv_0_3, base_filt*2, is_training = self.training)\n",
        "            conv_0_4 = tf.layers.dropout(conv_0_4, rate = self.drop, training = self.training)\n",
        "            conv_out = tf.layers.conv3d(conv_0_4, OUTPUT_CLASSES, [1,1,1], [1,1,1], padding = 'same')\n",
        "            self.predictions = tf.expand_dims(tf.argmax(conv_out, axis = -1), -1)\n",
        "            \n",
        "            # Note, this can be more easily visualised in a tool like tensorboard; Follows exact same format as in Paper.\n",
        "            \n",
        "            if self.do_print: \n",
        "                print('Model Convolution output shape', conv_out.get_shape())\n",
        "                print('Model Argmax output shape', self.predictions.get_shape())\n",
        "            \n",
        "            do_weight = True\n",
        "            loss_weights = [0.00439314, 0.68209101, 0.31351585] # see section 1.4 # instead of [1, 150, 100, 1.0] \n",
        "            # Weighted cross entropy: approach adapts following code: https://stackoverflow.com/questions/44560549/unbalanced-data-and-weighted-cross-entropy\n",
        "            ce_loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=conv_out, labels=labels_one_hot)\n",
        "            if do_weight:\n",
        "                weighted_loss = tf.reshape(tf.constant(loss_weights), [1, 1, 1, 1, num_classes]) # Format to the right size\n",
        "                weighted_one_hot = tf.reduce_sum(weighted_loss*labels_one_hot, axis = -1)\n",
        "                ce_loss = ce_loss * weighted_one_hot\n",
        "            self.loss = tf.reduce_mean(ce_loss) # Get loss\n",
        "            \n",
        "            self.trainer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "            \n",
        "            self.extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) # Ensure correct ordering for batch-norm to work\n",
        "            with tf.control_dependencies(self.extra_update_ops):\n",
        "                self.train_op = self.trainer.minimize(self.loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIARWZclMbtT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Op4ZOBWT66d"
      },
      "source": [
        "#model with attention module and dense layers:\n",
        "\n",
        "simpleUNet = False\n",
        "\n",
        "class UNetwork():\n",
        "    \n",
        "    def conv_batch_relu(self, tensor, filters, kernel = [3,3,3], stride = [1,1,1], is_training = True):\n",
        "        # Produces the conv_batch_relu combination as in the paper\n",
        "        padding = 'valid'\n",
        "        if self.should_pad: padding = 'same'\n",
        "    \n",
        "        conv = tf.layers.conv3d(tensor, filters, kernel_size = kernel, strides = stride, padding = padding,\n",
        "                                kernel_initializer = self.base_init, kernel_regularizer = self.reg_init)\n",
        "        conv = tf.layers.batch_normalization(conv, training = is_training)\n",
        "        conv = tf.nn.leaky_relu(conv)\n",
        "        #conv = tf.nn.relu(conv) \n",
        "        return conv\n",
        "    # densenet model    \n",
        "    def dense_conv_1(self, tensor, filters, kernel = [3,3,3], stride = [1,1,1], is_training = True):\n",
        "        conv_4_1 = self.conv_batch_relu(tensor, filters, is_training = self.training)\n",
        "        conv_4_2 = self.conv_batch_relu(conv_4_1, filters, is_training = self.training)\n",
        "        conv_4_2 = tf.layers.dropout(conv_4_2, rate = self.drop, training = self.training)\n",
        "\n",
        "        conv_5_1 = self.conv_batch_relu(conv_4_2, filters, is_training = self.training)\n",
        "        conv_5_2 = self.conv_batch_relu(conv_5_1, filters, is_training = self.training)\n",
        "        conv_5_2 = tf.layers.dropout(conv_5_2, rate = self.drop, training = self.training)\n",
        "\n",
        "        concate_merge = tf.concat((conv_4_2,conv_5_2),4)\n",
        "        conv_6_1 = self.conv_batch_relu(concate_merge, filters, is_training = self.training)\n",
        "        conv_6_2 = self.conv_batch_relu(conv_6_1, filters, is_training = self.training)\n",
        "        conv_6_2 = tf.layers.dropout(conv_6_2, rate = self.drop, training = self.training)\n",
        "\n",
        "        concate_merge1 = tf.concat((conv_4_2,conv_6_2),4)\n",
        "        concate_merge2 = tf.concat((conv_5_2,concate_merge1),4)\n",
        "        conv_7_1 = self.conv_batch_relu(concate_merge2, filters, is_training = self.training)\n",
        "        conv_7_2 = self.conv_batch_relu(conv_7_1, filters, is_training = self.training)\n",
        "        conv_7_2 = tf.layers.dropout(conv_7_2, rate = self.drop, training = self.training)\n",
        "        \n",
        "        return conv_7_2\n",
        "\n",
        "    def upconvolve(self, tensor, filters, kernel = 2, stride = 2, scale = 4, activation = None):\n",
        "        # Upconvolution - two different implementations: the first is as suggested in the original Unet paper and the second is a more recent version\n",
        "        # Needs to be determined if these do the same thing\n",
        "        padding = 'valid'\n",
        "        if self.should_pad: padding = 'same'\n",
        "        # upsample_routine = tf.keras.layers.UpSampling3D(size = (scale,scale,scale)) # Uses tf.resize_images\n",
        "        # tensor = upsample_routine(tensor)\n",
        "        # conv = tf.layers.conv3d(tensor, filters, kernel, stride, padding = 'same',\n",
        "        #                                 kernel_initializer = self.base_init, kernel_regularizer = self.reg_init)\n",
        "        # use_bias = False is a tensorflow bug\n",
        "        conv = tf.layers.conv3d_transpose(tensor, filters, kernel_size = kernel, strides = stride, padding = padding, use_bias=False, \n",
        "                                          kernel_initializer = self.base_init,  kernel_regularizer = self.reg_init)\n",
        "        return conv\n",
        "    #attention model on skip connection\n",
        "    def centre_crop_and_concat(self, prev_conv, up_conv,filters):\n",
        "        # If concatenating two different sized Tensors, centre crop the first Tensor to the right size and concat\n",
        "        # Needed if you don't have padding\n",
        "        p_c_s = prev_conv.get_shape()\n",
        "        u_c_s = up_conv.get_shape()\n",
        "        offsets =  np.array([0, (p_c_s[1] - u_c_s[1]) // 2, (p_c_s[2] - u_c_s[2]) // 2, \n",
        "                             (p_c_s[3] - u_c_s[3]) // 2, 0], dtype = np.int32)\n",
        "        size = np.array([-1, u_c_s[1], u_c_s[2], u_c_s[3], p_c_s[4]], np.int32)\n",
        "        prev_conv_crop = tf.slice(prev_conv, offsets, size)\n",
        "        conv_out1 = tf.layers.conv3d(prev_conv_crop,filters, [1,1,1], [1,1,1], padding = 'same')\n",
        "        conv_out2 = tf.layers.conv3d(up_conv,filters, [1,1,1], [1,1,1], padding = 'same')\n",
        "        add = tf.add(conv_out1,conv_out2)\n",
        "        conv = tf.nn.leaky_relu(add)\n",
        "        conv_out3 = tf.layers.conv3d(conv,filters, [1,1,1], [1,1,1], padding = 'same')\n",
        "        final_atten = tf.nn.sigmoid(conv_out3)\n",
        "        up_concat = tf.maximum(final_atten, up_conv)\n",
        "        #up_concat = tf.matmul(final_atten, up_conv)\n",
        "\n",
        "        #up_concat = tf.maximum((final_atten, up_conv), 4)\n",
        "        return up_concat\n",
        "        \n",
        "    def __init__(self, base_filt = 16, in_depth = INPUT_DEPTH, out_depth = OUTPUT_DEPTH,\n",
        "                 in_size = INPUT_SIZE, out_size = OUTPUT_SIZE, num_classes = OUTPUT_CLASSES,\n",
        "                 learning_rate = 0.001, print_shapes = True, drop = 0.2, should_pad = False):\n",
        "        # Initialise your model with the parameters defined above\n",
        "        # Print-shape is a debug shape printer for convenience\n",
        "        # Should_pad controls whether the model has padding or not\n",
        "        # Base_filt controls the number of base conv filters the model has. Note deeper analysis paths have filters that are scaled by this value\n",
        "        # Drop specifies the proportion of dropped activations\n",
        "        \n",
        "        self.base_init = tf.truncated_normal_initializer(stddev=0.1) # Initialise weights\n",
        "        self.reg_init = tf.contrib.layers.l2_regularizer(scale=0.1) # Initialise regularisation (was useful)\n",
        "        \n",
        "        self.should_pad = should_pad # To pad or not to pad, that is the question\n",
        "        self.drop = drop # Set dropout rate\n",
        "        \n",
        "        with tf.variable_scope('3DuNet'):\n",
        "            self.training = tf.placeholder(tf.bool)\n",
        "            self.do_print = print_shapes\n",
        "            self.model_input = tf.placeholder(tf.float32, shape = (None, in_depth, in_size, in_size, 1))  \n",
        "            # Define placeholders for feed_dict\n",
        "            self.model_labels = tf.placeholder(tf.int32, shape = (None, out_depth, out_size, out_size, 1))\n",
        "            labels_one_hot = tf.squeeze(tf.one_hot(self.model_labels, num_classes, axis = -1), axis = -2)\n",
        "            \n",
        "            if self.do_print: \n",
        "                print('Input features shape', self.model_input.get_shape())\n",
        "                print('Labels shape', labels_one_hot.get_shape())\n",
        "                \n",
        "            # Level zero\n",
        "            conv_0_1 = self.conv_batch_relu(self.model_input, base_filt, is_training = self.training)\n",
        "            conv_0_2 = self.conv_batch_relu(conv_0_1, base_filt*2, is_training = self.training)\n",
        "            dense_1 = self.dense_conv_1(conv_0_2, base_filt*2,is_training = self.training)\n",
        "           \n",
        "            # Level one\n",
        "            max_1_1 = tf.layers.max_pooling3d(dense_1, [1,2,2], [1,2,2]) # Stride, Kernel previously [2,2,2]\n",
        "            conv_1_1 = self.conv_batch_relu(max_1_1, base_filt*2, is_training = self.training)\n",
        "            conv_1_2 = self.conv_batch_relu(conv_1_1, base_filt*4, is_training = self.training)\n",
        "            dense_2 = self.dense_conv_1(conv_1_2, base_filt*4,is_training = self.training)\n",
        "            conv_1_2 = tf.layers.dropout(dense_2, rate = self.drop, training = self.training)\n",
        "            # Level two\n",
        "            max_2_1 = tf.layers.max_pooling3d(conv_1_2, [1,2,2], [1,2,2]) # Stride, Kernel previously [2,2,2]\n",
        "            conv_2_1 = self.conv_batch_relu(max_2_1, base_filt*4, is_training = self.training)\n",
        "            conv_2_2 = self.conv_batch_relu(conv_2_1, base_filt*8, is_training = self.training)\n",
        "            dense_3 = self.dense_conv_1(conv_2_2, base_filt*8,is_training = self.training)\n",
        "            conv_2_2 = tf.layers.dropout(dense_3, rate = self.drop, training = self.training)\n",
        "            \n",
        "            if simpleUNet:\n",
        "                # Level one\n",
        "                up_conv_2_1 = self.upconvolve(conv_2_2, base_filt*8, kernel = 2, stride = [1,2,2]) # Stride previously [2,2,2]\n",
        "            else:\n",
        "                # Level three\n",
        "                max_3_1 = tf.layers.max_pooling3d(conv_2_2, [1,2,2], [1,2,2]) # Stride, Kernel previously [2,2,2]\n",
        "                conv_3_1 = self.conv_batch_relu(max_3_1, base_filt*8, is_training = self.training)\n",
        "                conv_3_2 = self.conv_batch_relu(conv_3_1, base_filt*16, is_training = self.training)\n",
        "                dense_4 = self.dense_conv_1(conv_3_2, base_filt*16,is_training = self.training)\n",
        "                conv_3_2 = tf.layers.dropout(dense_4, rate = self.drop, training = self.training)\n",
        "                conv_atten = tf.layers.conv3d(conv_3_2,base_filt*16, [1,1,1], [1,1,1], padding = 'same')\n",
        "                conv_atten = tf.nn.leaky_relu(conv_atten)\n",
        "                conv_atten = tf.layers.conv3d(conv_atten,base_filt*16, [1,1,1], [1,1,1], padding = 'same')\n",
        "                final_atten = tf.nn.sigmoid(conv_atten)\n",
        "                up_concat = tf.multiply(final_atten, conv_3_2)\n",
        "                # Level two\n",
        "                up_conv_3_2 = self.upconvolve(conv_3_2, base_filt*16, kernel = 2, stride = [1,2,2]) # Stride previously [2,2,2] \n",
        "                concat_2_1 = self.centre_crop_and_concat(conv_2_2, up_conv_3_2,base_filt*16)\n",
        "                conv_2_3 = self.conv_batch_relu(concat_2_1, base_filt*8, is_training = self.training)\n",
        "                conv_2_4 = self.conv_batch_relu(conv_2_3, base_filt*8, is_training = self.training)\n",
        "                dense_5 = self.dense_conv_1(conv_2_4, base_filt*8,is_training = self.training)\n",
        "                conv_2_4 = tf.layers.dropout(dense_5, rate = self.drop, training = self.training)\n",
        "                # Level one\n",
        "                up_conv_2_1 = self.upconvolve(conv_2_4, base_filt*8, kernel = 2, stride = [1,2,2]) # Stride previously [2,2,2]\n",
        "            \n",
        "            concat_1_1 = self.centre_crop_and_concat(conv_1_2, up_conv_2_1,base_filt*8)\n",
        "            conv_1_3 = self.conv_batch_relu(concat_1_1, base_filt*4, is_training = self.training)\n",
        "            conv_1_4 = self.conv_batch_relu(conv_1_3, base_filt*4, is_training = self.training)\n",
        "            dense_6 = self.dense_conv_1(conv_1_4, base_filt*4,is_training = self.training)\n",
        "            conv_1_4 = tf.layers.dropout(dense_6, rate = self.drop, training = self.training)\n",
        "            # Level zero\n",
        "            up_conv_1_0 = self.upconvolve(conv_1_4, base_filt*4, kernel = 2, stride = [1,2,2])  # Stride previously [2,2,2]\n",
        "            concat_0_1 = self.centre_crop_and_concat(conv_0_2, up_conv_1_0,base_filt*4)\n",
        "            conv_0_3 = self.conv_batch_relu(concat_0_1, base_filt*2, is_training = self.training)\n",
        "            conv_0_4 = self.conv_batch_relu(conv_0_3, base_filt*2, is_training = self.training)\n",
        "            dense_7 = self.dense_conv_1(conv_0_4, base_filt*2,is_training = self.training)\n",
        "            conv_0_4 = tf.layers.dropout(dense_7, rate = self.drop, training = self.training)\n",
        "            conv_out = tf.layers.conv3d(conv_0_4, OUTPUT_CLASSES, [1,1,1], [1,1,1], padding = 'same')\n",
        "            self.predictions = tf.expand_dims(tf.argmax(conv_out, axis = -1), -1)\n",
        "            \n",
        "            # Note, this can be more easily visualised in a tool like tensorboard; Follows exact same format as in Paper.\n",
        "            \n",
        "            if self.do_print: \n",
        "                print('Model Convolution output shape', conv_out.get_shape())\n",
        "                print('Model Argmax output shape', self.predictions.get_shape())\n",
        "            \n",
        "            do_weight = True\n",
        "            loss_weights = [0.00439314, 0.68209101, 0.31351585] # see section 1.4 # instead of [1, 150, 100, 1.0] \n",
        "            # Weighted cross entropy: approach adapts following code: https://stackoverflow.com/questions/44560549/unbalanced-data-and-weighted-cross-entropy\n",
        "            ce_loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=conv_out, labels=labels_one_hot)\n",
        "            if do_weight:\n",
        "                weighted_loss = tf.reshape(tf.constant(loss_weights), [1, 1, 1, 1, num_classes]) # Format to the right size\n",
        "                weighted_one_hot = tf.reduce_sum(weighted_loss*labels_one_hot, axis = -1)\n",
        "                ce_loss = ce_loss * weighted_one_hot\n",
        "            self.loss = tf.reduce_mean(ce_loss) # Get loss\n",
        "            \n",
        "            self.trainer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "            \n",
        "            self.extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) # Ensure correct ordering for batch-norm to work\n",
        "            with tf.control_dependencies(self.extra_update_ops):\n",
        "                self.train_op = self.trainer.minimize(self.loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sna9Y1i4Mubl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdDiqpqCH9mG"
      },
      "source": [
        "#model with attention module:\n",
        "\n",
        "simpleUNet = False\n",
        "\n",
        "class UNetwork():\n",
        "    \n",
        "    def conv_batch_relu(self, tensor, filters, kernel = [3,3,3], stride = [1,1,1], is_training = True):\n",
        "        # Produces the conv_batch_relu combination as in the paper\n",
        "        padding = 'valid'\n",
        "        if self.should_pad: padding = 'same'\n",
        "    \n",
        "        conv = tf.layers.conv3d(tensor, filters, kernel_size = kernel, strides = stride, padding = padding,\n",
        "                                kernel_initializer = self.base_init, kernel_regularizer = self.reg_init)\n",
        "        conv = tf.layers.batch_normalization(conv, training = is_training)\n",
        "        conv = tf.nn.leaky_relu(conv)\n",
        "        #conv = tf.nn.relu(conv) \n",
        "        return conv\n",
        "\n",
        "    def upconvolve(self, tensor, filters, kernel = 2, stride = 2, scale = 4, activation = None):\n",
        "        # Upconvolution - two different implementations: the first is as suggested in the original Unet paper and the second is a more recent version\n",
        "        # Needs to be determined if these do the same thing\n",
        "        padding = 'valid'\n",
        "        if self.should_pad: padding = 'same'\n",
        "        # upsample_routine = tf.keras.layers.UpSampling3D(size = (scale,scale,scale)) # Uses tf.resize_images\n",
        "        # tensor = upsample_routine(tensor)\n",
        "        # conv = tf.layers.conv3d(tensor, filters, kernel, stride, padding = 'same',\n",
        "        #                                 kernel_initializer = self.base_init, kernel_regularizer = self.reg_init)\n",
        "        # use_bias = False is a tensorflow bug\n",
        "        conv = tf.layers.conv3d_transpose(tensor, filters, kernel_size = kernel, strides = stride, padding = padding, use_bias=False, \n",
        "                                          kernel_initializer = self.base_init,  kernel_regularizer = self.reg_init)\n",
        "        return conv\n",
        "\n",
        "    def centre_crop_and_concat(self, prev_conv, up_conv,filters):\n",
        "        # If concatenating two different sized Tensors, centre crop the first Tensor to the right size and concat\n",
        "        # Needed if you don't have padding\n",
        "        p_c_s = prev_conv.get_shape()\n",
        "        u_c_s = up_conv.get_shape()\n",
        "        offsets =  np.array([0, (p_c_s[1] - u_c_s[1]) // 2, (p_c_s[2] - u_c_s[2]) // 2, \n",
        "                             (p_c_s[3] - u_c_s[3]) // 2, 0], dtype = np.int32)\n",
        "        size = np.array([-1, u_c_s[1], u_c_s[2], u_c_s[3], p_c_s[4]], np.int32)\n",
        "        prev_conv_crop = tf.slice(prev_conv, offsets, size)\n",
        "        conv_out1 = tf.layers.conv3d(prev_conv_crop,filters, [1,1,1], [1,1,1], padding = 'same')\n",
        "        conv_out2 = tf.layers.conv3d(up_conv,filters, [1,1,1], [1,1,1], padding = 'same')\n",
        "        add = tf.add(conv_out1,conv_out2)\n",
        "        conv = tf.nn.leaky_relu(add)\n",
        "        conv_out3 = tf.layers.conv3d(conv,filters, [1,1,1], [1,1,1], padding = 'same')\n",
        "        final_atten = tf.nn.sigmoid(conv_out3)\n",
        "        up_concat = tf.matmul(final_atten, up_conv)\n",
        "\n",
        "        #up_concat = tf.concat((final_atten, up_conv), 4)\n",
        "        return up_concat\n",
        "        \n",
        "    def __init__(self, base_filt = 16, in_depth = INPUT_DEPTH, out_depth = OUTPUT_DEPTH,\n",
        "                 in_size = INPUT_SIZE, out_size = OUTPUT_SIZE, num_classes = OUTPUT_CLASSES,\n",
        "                 learning_rate = 0.001, print_shapes = True, drop = 0.2, should_pad = False):\n",
        "        # Initialise your model with the parameters defined above\n",
        "        # Print-shape is a debug shape printer for convenience\n",
        "        # Should_pad controls whether the model has padding or not\n",
        "        # Base_filt controls the number of base conv filters the model has. Note deeper analysis paths have filters that are scaled by this value\n",
        "        # Drop specifies the proportion of dropped activations\n",
        "        \n",
        "        self.base_init = tf.truncated_normal_initializer(stddev=0.1) # Initialise weights\n",
        "        self.reg_init = tf.contrib.layers.l2_regularizer(scale=0.1) # Initialise regularisation (was useful)\n",
        "        \n",
        "        self.should_pad = should_pad # To pad or not to pad, that is the question\n",
        "        self.drop = drop # Set dropout rate\n",
        "        \n",
        "        with tf.variable_scope('3DuNet'):\n",
        "            self.training = tf.placeholder(tf.bool)\n",
        "            self.do_print = print_shapes\n",
        "            self.model_input = tf.placeholder(tf.float32, shape = (None, in_depth, in_size, in_size, 1))  \n",
        "            # Define placeholders for feed_dict\n",
        "            self.model_labels = tf.placeholder(tf.int32, shape = (None, out_depth, out_size, out_size, 1))\n",
        "            labels_one_hot = tf.squeeze(tf.one_hot(self.model_labels, num_classes, axis = -1), axis = -2)\n",
        "            \n",
        "            if self.do_print: \n",
        "                print('Input features shape', self.model_input.get_shape())\n",
        "                print('Labels shape', labels_one_hot.get_shape())\n",
        "                \n",
        "            # Level zero\n",
        "            conv_0_1 = self.conv_batch_relu(self.model_input, base_filt, is_training = self.training)\n",
        "            conv_0_2 = self.conv_batch_relu(conv_0_1, base_filt*2, is_training = self.training)\n",
        "            # Level one\n",
        "            max_1_1 = tf.layers.max_pooling3d(conv_0_2, [1,2,2], [1,2,2]) # Stride, Kernel previously [2,2,2]\n",
        "            conv_1_1 = self.conv_batch_relu(max_1_1, base_filt*2, is_training = self.training)\n",
        "            conv_1_2 = self.conv_batch_relu(conv_1_1, base_filt*4, is_training = self.training)\n",
        "            conv_1_2 = tf.layers.dropout(conv_1_2, rate = self.drop, training = self.training)\n",
        "            # Level two\n",
        "            max_2_1 = tf.layers.max_pooling3d(conv_1_2, [1,2,2], [1,2,2]) # Stride, Kernel previously [2,2,2]\n",
        "            conv_2_1 = self.conv_batch_relu(max_2_1, base_filt*4, is_training = self.training)\n",
        "            conv_2_2 = self.conv_batch_relu(conv_2_1, base_filt*8, is_training = self.training)\n",
        "            conv_2_2 = tf.layers.dropout(conv_2_2, rate = self.drop, training = self.training)\n",
        "            \n",
        "            if simpleUNet:\n",
        "                # Level one\n",
        "                up_conv_2_1 = self.upconvolve(conv_2_2, base_filt*8, kernel = 2, stride = [1,2,2]) # Stride previously [2,2,2]\n",
        "            else:\n",
        "                # Level three\n",
        "                max_3_1 = tf.layers.max_pooling3d(conv_2_2, [1,2,2], [1,2,2]) # Stride, Kernel previously [2,2,2]\n",
        "                conv_3_1 = self.conv_batch_relu(max_3_1, base_filt*8, is_training = self.training)\n",
        "                conv_3_2 = self.conv_batch_relu(conv_3_1, base_filt*16, is_training = self.training)\n",
        "                conv_3_2 = tf.layers.dropout(conv_3_2, rate = self.drop, training = self.training)\n",
        "                # Level two\n",
        "                up_conv_3_2 = self.upconvolve(conv_3_2, base_filt*16, kernel = 2, stride = [1,2,2]) # Stride previously [2,2,2] \n",
        "                concat_2_1 = self.centre_crop_and_concat(conv_2_2, up_conv_3_2,base_filt*16)\n",
        "                conv_2_3 = self.conv_batch_relu(concat_2_1, base_filt*8, is_training = self.training)\n",
        "                conv_2_4 = self.conv_batch_relu(conv_2_3, base_filt*8, is_training = self.training)\n",
        "                conv_2_4 = tf.layers.dropout(conv_2_4, rate = self.drop, training = self.training)\n",
        "                # Level one\n",
        "                up_conv_2_1 = self.upconvolve(conv_2_4, base_filt*8, kernel = 2, stride = [1,2,2]) # Stride previously [2,2,2]\n",
        "            \n",
        "            concat_1_1 = self.centre_crop_and_concat(conv_1_2, up_conv_2_1,base_filt*8)\n",
        "            conv_1_3 = self.conv_batch_relu(concat_1_1, base_filt*4, is_training = self.training)\n",
        "            conv_1_4 = self.conv_batch_relu(conv_1_3, base_filt*4, is_training = self.training)\n",
        "            conv_1_4 = tf.layers.dropout(conv_1_4, rate = self.drop, training = self.training)\n",
        "            # Level zero\n",
        "            up_conv_1_0 = self.upconvolve(conv_1_4, base_filt*4, kernel = 2, stride = [1,2,2])  # Stride previously [2,2,2]\n",
        "            concat_0_1 = self.centre_crop_and_concat(conv_0_2, up_conv_1_0,base_filt*4)\n",
        "            conv_0_3 = self.conv_batch_relu(concat_0_1, base_filt*2, is_training = self.training)\n",
        "            conv_0_4 = self.conv_batch_relu(conv_0_3, base_filt*2, is_training = self.training)\n",
        "            conv_0_4 = tf.layers.dropout(conv_0_4, rate = self.drop, training = self.training)\n",
        "            conv_out = tf.layers.conv3d(conv_0_4, OUTPUT_CLASSES, [1,1,1], [1,1,1], padding = 'same')\n",
        "            self.predictions = tf.expand_dims(tf.argmax(conv_out, axis = -1), -1)\n",
        "            \n",
        "            # Note, this can be more easily visualised in a tool like tensorboard; Follows exact same format as in Paper.\n",
        "            \n",
        "            if self.do_print: \n",
        "                print('Model Convolution output shape', conv_out.get_shape())\n",
        "                print('Model Argmax output shape', self.predictions.get_shape())\n",
        "            \n",
        "            do_weight = True\n",
        "            loss_weights = [0.00439314, 0.68209101, 0.31351585] # see section 1.4 # instead of [1, 150, 100, 1.0] \n",
        "            # Weighted cross entropy: approach adapts following code: https://stackoverflow.com/questions/44560549/unbalanced-data-and-weighted-cross-entropy\n",
        "            ce_loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=conv_out, labels=labels_one_hot)\n",
        "            if do_weight:\n",
        "                weighted_loss = tf.reshape(tf.constant(loss_weights), [1, 1, 1, 1, num_classes]) # Format to the right size\n",
        "                weighted_one_hot = tf.reduce_sum(weighted_loss*labels_one_hot, axis = -1)\n",
        "                ce_loss = ce_loss * weighted_one_hot\n",
        "            self.loss = tf.reduce_mean(ce_loss) # Get loss\n",
        "            \n",
        "            self.trainer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "            \n",
        "            self.extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) # Ensure correct ordering for batch-norm to work\n",
        "            with tf.control_dependencies(self.extra_update_ops):\n",
        "                self.train_op = self.trainer.minimize(self.loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8PYU806NTYB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0mRkhE0UBif"
      },
      "source": [
        "# 3DUnet with the resnet34 on encoder side\n",
        "\n",
        "simpleUNet = False\n",
        "\n",
        "class UNetwork():\n",
        "\n",
        "    def conv_batch_relu(self, tensor, filters, kernel = [3,3,3], stride = [1,1,1], is_training = True):\n",
        "        # Produces the conv_batch_relu combination as in the paper\n",
        "        padding = 'valid'\n",
        "        if self.should_pad: padding = 'same'\n",
        "    \n",
        "        conv = tf.layers.conv3d(tensor, filters, kernel_size = kernel, strides = stride, padding = padding,\n",
        "                                kernel_initializer = self.base_init, kernel_regularizer = self.reg_init)\n",
        "        conv = tf.layers.batch_normalization(conv, training = is_training)\n",
        "        conv = tf.nn.relu(conv) \n",
        "        return conv\n",
        "    def conv_pool(self, tensor, filters, kernel = [3,3,3], stride = [1,1,1], is_training = True):\n",
        "        # Produces the conv_batch_relu combination as in the paper\n",
        "        padding = 'valid'\n",
        "        if self.should_pad: padding = 'same'\n",
        "    \n",
        "        conv = tf.layers.conv3d(tensor, filters, kernel_size = kernel, strides = stride, padding = padding,\n",
        "                                kernel_initializer = self.base_init, kernel_regularizer = self.reg_init)\n",
        "        #conv = tf.layers.batch_normalization(conv, training = is_training)\n",
        "        #conv = tf.nn.relu(conv) \n",
        "        max = tf.layers.max_pooling3d(conv, [1,2,2], [1,2,2])\n",
        "        return max\n",
        "\n",
        "    def conv_batch_relu_conv(self, tensor, filters, kernel = [3,3,3], stride = [1,1,1], is_training = True):\n",
        "        # Produces the conv_batch_relu combination as in the paper\n",
        "        padding = 'valid'\n",
        "        if self.should_pad: padding = 'same'\n",
        "        conv = tf.layers.batch_normalization(tensor, training = is_training)\n",
        "        conv = tf.nn.relu(conv)\n",
        "    \n",
        "        conv = tf.layers.conv3d(conv, filters, kernel_size = kernel, strides = stride, padding = padding,\n",
        "                                kernel_initializer = self.base_init, kernel_regularizer = self.reg_init)\n",
        "        conv = tf.layers.batch_normalization(conv, training = is_training)\n",
        "        conv = tf.nn.relu(conv)\n",
        "        conv = tf.layers.conv3d(conv, filters, kernel_size = kernel, strides = stride, padding = padding,\n",
        "                                kernel_initializer = self.base_init, kernel_regularizer = self.reg_init) \n",
        "        return conv\n",
        "    def conv_relu(self, tensor, filters, kernel = [3,3,3], stride = [1,1,1], is_training = True):\n",
        "        # Produces the conv_batch_relu combination as in the paper\n",
        "        padding = 'valid'\n",
        "        if self.should_pad: padding = 'same'\n",
        "    \n",
        "        conv = tf.layers.conv3d(tensor, filters, kernel_size = kernel, strides = stride, padding = padding,\n",
        "                                kernel_initializer = self.base_init, kernel_regularizer = self.reg_init)\n",
        "        #conv = tf.layers.batch_normalization(conv, training = is_training)\n",
        "        conv = tf.nn.relu(conv) \n",
        "\n",
        "        conv = tf.layers.conv3d(tensor, filters, kernel_size = kernel, strides = stride, padding = padding,\n",
        "                                kernel_initializer = self.base_init, kernel_regularizer = self.reg_init)\n",
        "        #conv = tf.layers.batch_normalization(conv, training = is_training)\n",
        "        conv = tf.nn.relu(conv)\n",
        "        return conv    \n",
        "\n",
        "\n",
        "    def upconvolve(self, tensor, filters, kernel = 2, stride = 2, scale = 4, activation = None):\n",
        "        # Upconvolution - two different implementations: the first is as suggested in the original Unet paper and the second is a more recent version\n",
        "        # Needs to be determined if these do the same thing\n",
        "        padding = 'valid'\n",
        "        if self.should_pad: padding = 'same'\n",
        "        # upsample_routine = tf.keras.layers.UpSampling3D(size = (scale,scale,scale)) # Uses tf.resize_images\n",
        "        # tensor = upsample_routine(tensor)\n",
        "        # conv = tf.layers.conv3d(tensor, filters, kernel, stride, padding = 'same',\n",
        "        #                                 kernel_initializer = self.base_init, kernel_regularizer = self.reg_init)\n",
        "        # use_bias = False is a tensorflow bug\n",
        "        conv = tf.layers.conv3d_transpose(tensor, filters, kernel_size = kernel, strides = stride, padding = padding, use_bias=False, \n",
        "                                          kernel_initializer = self.base_init,  kernel_regularizer = self.reg_init)\n",
        "        return conv\n",
        "\n",
        "    def centre_crop_and_concat(self, prev_conv, up_conv):\n",
        "        # If concatenating two different sized Tensors, centre crop the first Tensor to the right size and concat\n",
        "        # Needed if you don't have padding\n",
        "        p_c_s = prev_conv.get_shape()\n",
        "        u_c_s = up_conv.get_shape()\n",
        "        offsets =  np.array([0, (p_c_s[1] - u_c_s[1]) // 2, (p_c_s[2] - u_c_s[2]) // 2, \n",
        "                             (p_c_s[3] - u_c_s[3]) // 2, 0], dtype = np.int32)\n",
        "        size = np.array([-1, u_c_s[1], u_c_s[2], u_c_s[3], p_c_s[4]], np.int32)\n",
        "        prev_conv_crop = tf.slice(prev_conv, offsets, size)\n",
        "        up_concat = tf.concat((prev_conv_crop, up_conv), 4)\n",
        "        return up_concat\n",
        "        \n",
        "    def __init__(self, base_filt = 8, in_depth = INPUT_DEPTH, out_depth = OUTPUT_DEPTH,\n",
        "                 in_size = INPUT_SIZE, out_size = OUTPUT_SIZE, num_classes = OUTPUT_CLASSES,\n",
        "                 learning_rate = 0.001, print_shapes = True, drop = 0.2, should_pad = True):\n",
        "        # Initialise your model with the parameters defined above\n",
        "        # Print-shape is a debug shape printer for convenience\n",
        "        # Should_pad controls whether the model has padding or not\n",
        "        # Base_filt controls the number of base conv filters the model has. Note deeper analysis paths have filters that are scaled by this value\n",
        "        # Drop specifies the proportion of dropped activations\n",
        "        \n",
        "        self.base_init = tf.truncated_normal_initializer(stddev=0.1) # Initialise weights\n",
        "        self.reg_init = tf.contrib.layers.l2_regularizer(scale=0.1) # Initialise regularisation (was useful)\n",
        "        \n",
        "        self.should_pad = should_pad # To pad or not to pad, that is the question\n",
        "        self.drop = drop # Set dropout rate\n",
        "        \n",
        "        with tf.variable_scope('3DuNet'):\n",
        "            self.training = tf.placeholder(tf.bool)\n",
        "            self.do_print = print_shapes\n",
        "            self.model_input = tf.placeholder(tf.float32, shape = (None, in_depth, in_size, in_size, 1))  \n",
        "            # Define placeholders for feed_dict\n",
        "            self.model_labels = tf.placeholder(tf.int32, shape = (None, out_depth, out_size, out_size, 1))\n",
        "            labels_one_hot = tf.squeeze(tf.one_hot(self.model_labels, num_classes, axis = -1), axis = -2)\n",
        "            \n",
        "            if self.do_print: \n",
        "                print('Input features shape', self.model_input.get_shape())\n",
        "                print('Labels shape', labels_one_hot.get_shape())\n",
        "                \n",
        "            # Level zero\n",
        "            conv_0_1 = self.conv_batch_relu(self.model_input, base_filt, is_training = self.training)\n",
        "            conv_0_2 = self.conv_batch_relu(conv_0_1, base_filt*2, is_training = self.training)\n",
        "            # Level one\n",
        "            max_1_1 = tf.layers.max_pooling3d(conv_0_2, [1,2,2], [1,2,2]) # Stride, Kernel previously [2,2,2]\n",
        "\n",
        "            conv_1_1 = self.conv_batch_relu_conv(max_1_1, base_filt*2, is_training = self.training)\n",
        "            add_1 = tf.add(max_1_1,conv_1_1)\n",
        "\n",
        "            conv_2_1 = self.conv_batch_relu_conv(add_1, base_filt*2, is_training = self.training)\n",
        "            add_2 = tf.add(add_1,conv_2_1)\n",
        "\n",
        "            conv_3_1 = self.conv_batch_relu_conv(add_2, base_filt*2, is_training = self.training)\n",
        "            add_3 = tf.add(add_2,conv_3_1)\n",
        "\n",
        "            conv_4_1 = self.conv_batch_relu_conv(add_3, base_filt*4, is_training = self.training)\n",
        "            #here 1st concate\n",
        "            max_2_1 = tf.layers.max_pooling3d(conv_4_1, [1,2,2], [1,2,2])\n",
        "\n",
        "            skip_1 = self.conv_pool(add_3, base_filt*4, is_training = self.training)\n",
        "\n",
        "            add_4 = tf.add(max_2_1,skip_1)\n",
        "\n",
        "            conv_5_1 = self.conv_batch_relu_conv(add_4, base_filt*4, is_training = self.training)\n",
        "            add_5 = tf.add(add_4,conv_5_1)\n",
        "\n",
        "            conv_6_1 = self.conv_batch_relu_conv(add_5, base_filt*4, is_training = self.training)\n",
        "            add_6 = tf.add(add_5,conv_6_1)\n",
        "\n",
        "            conv_7_1 = self.conv_batch_relu_conv(add_6, base_filt*4, is_training = self.training)\n",
        "            add_7 = tf.add(add_6,conv_7_1)\n",
        "\n",
        "            conv_8_1 = self.conv_batch_relu_conv(add_7, base_filt*8, is_training = self.training)\n",
        "            #here 2st concate\n",
        "            max_3_1 = tf.layers.max_pooling3d(conv_8_1, [1,2,2], [1,2,2])\n",
        "\n",
        "            skip_2 = self.conv_pool(add_7, base_filt*8, is_training = self.training)\n",
        "\n",
        "            add_8 = tf.add(max_3_1,skip_2)\n",
        "\n",
        "            conv_9_1 = self.conv_batch_relu_conv(add_8, base_filt*8, is_training = self.training)\n",
        "            add_9 = tf.add(add_8,conv_9_1)\n",
        "\n",
        "            conv_10_1 = self.conv_batch_relu_conv(add_9, base_filt*8, is_training = self.training)\n",
        "            add_10 = tf.add(add_9,conv_10_1)\n",
        "\n",
        "            conv_11_1 = self.conv_batch_relu_conv(add_10, base_filt*8, is_training = self.training)\n",
        "            add_11 = tf.add(add_10,conv_11_1)\n",
        "\n",
        "            conv_12_1 = self.conv_batch_relu_conv(add_11, base_filt*8, is_training = self.training)\n",
        "            add_12 = tf.add(add_11,conv_12_1)\n",
        "            print(add_12.shape)\n",
        "\n",
        "            conv_13_1 = self.conv_batch_relu_conv(add_12, base_filt*8, is_training = self.training)\n",
        "            add_13 = tf.add(add_12,conv_13_1)\n",
        "\n",
        "            conv_14_1 = self.conv_batch_relu_conv(add_13, base_filt*16, is_training = self.training)\n",
        "            #here 2st concate\n",
        "            max_4_1 = tf.layers.max_pooling3d(conv_14_1, [1,2,2], [1,2,2])\n",
        "\n",
        "            skip_3 = self.conv_pool(add_13, base_filt*16, is_training = self.training)\n",
        "\n",
        "            add_14 = tf.add(max_4_1,skip_3)\n",
        "\n",
        "            conv_15_1 = self.conv_batch_relu_conv(add_14, base_filt*16, is_training = self.training)\n",
        "            add_15 = tf.add(add_14,conv_15_1)\n",
        "\n",
        "            conv_16_1 = self.conv_batch_relu_conv(add_15, base_filt*16, is_training = self.training)\n",
        "            add_16 = tf.add(add_15,conv_16_1)\n",
        "\n",
        "            batch_16 = tf.layers.batch_normalization(add_16, training= self.training)\n",
        "            last = tf.nn.relu(batch_16)\n",
        "\n",
        "            up_conv_3_2 = self.upconvolve(last, base_filt*16, kernel = 2, stride = [1,2,2])\n",
        "            concate_3 = tf.concat((up_conv_3_2,conv_14_1),4)\n",
        "            conv_17_1 = self.conv_relu(concate_3,base_filt*16, is_training = self.training)\n",
        "\n",
        "            up_conv_2_2 = self.upconvolve(conv_17_1, base_filt*8, kernel = 2, stride = [1,2,2])\n",
        "            concate_2 = tf.concat((up_conv_2_2,conv_8_1),4)\n",
        "            conv_18_1 = self.conv_relu(concate_2,base_filt*8, is_training = self.training)\n",
        "\n",
        "            up_conv_1_2 = self.upconvolve(conv_18_1, base_filt*4, kernel = 2, stride = [1,2,2])\n",
        "            concate_1 = tf.concat((up_conv_1_2,conv_4_1),4)\n",
        "            conv_19_1 = self.conv_relu(concate_1,base_filt*4, is_training = self.training)\n",
        "\n",
        "            \n",
        "            up_conv_0_2 = self.upconvolve(conv_19_1, base_filt*2, kernel = 2, stride = [1,2,2])\n",
        "            conv_20_1 = self.conv_relu(up_conv_0_2,base_filt*2, is_training = self.training)\n",
        "\n",
        "            conv_out = tf.layers.conv3d(conv_20_1, OUTPUT_CLASSES, [1,1,1], [1,1,1], padding = 'same')\n",
        "            self.predictions = tf.expand_dims(tf.argmax(conv_out, axis = -1), -1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            #conv_1_1 = self.conv_batch_relu(max_1_1, base_filt*2, is_training = self.training)\n",
        "            #conv_1_2 = self.conv_batch_relu(conv_1_1, base_filt*4, is_training = self.training)\n",
        "            #conv_1_2 = tf.layers.dropout(conv_1_2, rate = self.drop, training = self.training)\n",
        "            # Level two\n",
        "            #max_2_1 = tf.layers.max_pooling3d(conv_1_2, [1,2,2], [1,2,2]) # Stride, Kernel previously [2,2,2]\n",
        "            #conv_2_1 = self.conv_batch_relu(max_2_1, base_filt*4, is_training = self.training)\n",
        "            #conv_2_2 = self.conv_batch_relu(conv_2_1, base_filt*8, is_training = self.training)\n",
        "            #conv_2_2 = tf.layers.dropout(conv_2_2, rate = self.drop, training = self.training)\n",
        "            \n",
        "            #if simpleUNet:\n",
        "                # Level one\n",
        "             #   up_conv_2_1 = self.upconvolve(conv_2_2, base_filt*8, kernel = 2, stride = [1,2,2]) # Stride previously [2,2,2]\n",
        "            #else:\n",
        "                # Level three\n",
        "            #    max_3_1 = tf.layers.max_pooling3d(conv_2_2, [1,2,2], [1,2,2]) # Stride, Kernel previously [2,2,2]\n",
        "             #   conv_3_1 = self.conv_batch_relu(max_3_1, base_filt*8, is_training = self.training)\n",
        "              #  conv_3_2 = self.conv_batch_relu(conv_3_1, base_filt*16, is_training = self.training)\n",
        "               # conv_3_2 = tf.layers.dropout(conv_3_2, rate = self.drop, training = self.training)\n",
        "\n",
        "                #conv_4_1 = self.conv_batch_relu(conv_3_2, base_filt*16, is_training = self.training)\n",
        "                #conv_4_2 = self.conv_batch_relu(conv_4_1, base_filt*16, is_training = self.training)\n",
        "                #conv_4_2 = tf.layers.dropout(conv_4_2, rate = self.drop, training = self.training)\n",
        "\n",
        "                #conv_5_1 = self.conv_batch_relu(conv_4_2, base_filt*16, is_training = self.training)\n",
        "                #conv_5_2 = self.conv_batch_relu(conv_5_1, base_filt*16, is_training = self.training)\n",
        "                #conv_5_2 = tf.layers.dropout(conv_5_2, rate = self.drop, training = self.training)\n",
        "\n",
        "                #concate_merge = tf.concat((conv_4_2,conv_5_2),4)\n",
        "                #conv_6_1 = self.conv_batch_relu(concate_merge, base_filt*16, is_training = self.training)\n",
        "                #conv_6_2 = self.conv_batch_relu(conv_6_1, base_filt*16, is_training = self.training)\n",
        "                #conv_6_2 = tf.layers.dropout(conv_6_2, rate = self.drop, training = self.training)\n",
        "\n",
        "                #concate_merge1 = tf.concat((conv_4_2,conv_6_2),4)\n",
        "                #concate_merge2 = tf.concate((conv_5_2,concate_merge1),4)\n",
        "                #conv_7_1 = self.conv_batch_relu(concate_merge2, base_filt*16, is_training = self.training)\n",
        "                #conv_7_2 = self.conv_batch_relu(conv_7_1, base_filt*16, is_training = self.training)\n",
        "                #conv_7_2 = tf.layers.dropout(conv_7_2, rate = self.drop, training = self.training)\n",
        "\n",
        "\n",
        "                \n",
        "\n",
        "                # Level two\n",
        "                #up_conv_3_2 = self.upconvolve(conv_7_2, base_filt*16, kernel = 2, stride = [1,2,2]) # Stride previously [2,2,2] \n",
        "                #concat_2_1 = self.centre_crop_and_concat(conv_2_2, up_conv_3_2)\n",
        "                #conv_2_3 = self.conv_batch_relu(concat_2_1, base_filt*8, is_training = self.training)\n",
        "                #conv_2_4 = self.conv_batch_relu(conv_2_3, base_filt*8, is_training = self.training)\n",
        "                #conv_2_4 = tf.layers.dropout(conv_2_4, rate = self.drop, training = self.training)\n",
        "                # Level one\n",
        "                #up_conv_2_1 = self.upconvolve(conv_2_4, base_filt*8, kernel = 2, stride = [1,2,2]) # Stride previously [2,2,2]\n",
        "            \n",
        "            #concat_1_1 = self.centre_crop_and_concat(conv_1_2, up_conv_2_1)\n",
        "            #conv_1_3 = self.conv_batch_relu(concat_1_1, base_filt*4, is_training = self.training)\n",
        "            #conv_1_4 = self.conv_batch_relu(conv_1_3, base_filt*4, is_training = self.training)\n",
        "            #conv_1_4 = tf.layers.dropout(conv_1_4, rate = self.drop, training = self.training)\n",
        "            # Level zero\n",
        "            #up_conv_1_0 = self.upconvolve(conv_1_4, base_filt*4, kernel = 2, stride = [1,2,2])  # Stride previously [2,2,2]\n",
        "            #concat_0_1 = self.centre_crop_and_concat(conv_0_2, up_conv_1_0)\n",
        "            #conv_0_3 = self.conv_batch_relu(concat_0_1, base_filt*2, is_training = self.training)\n",
        "            #conv_0_4 = self.conv_batch_relu(conv_0_3, base_filt*2, is_training = self.training)\n",
        "            #conv_0_4 = tf.layers.dropout(conv_0_4, rate = self.drop, training = self.training)\n",
        "            #conv_out = tf.layers.conv3d(conv_0_4, OUTPUT_CLASSES, [1,1,1], [1,1,1], padding = 'same')\n",
        "            #self.predictions = tf.expand_dims(tf.argmax(conv_out, axis = -1), -1)\n",
        "            \n",
        "            # Note, this can be more easily visualised in a tool like tensorboard; Follows exact same format as in Paper.\n",
        "            \n",
        "            if self.do_print: \n",
        "                print('Model Convolution output shape', conv_out.get_shape())\n",
        "                print('Model Argmax output shape', self.predictions.get_shape())\n",
        "            \n",
        "            do_weight = True\n",
        "            loss_weights = [0.00439314, 0.68209101, 0.31351585] # see section 1.4 # instead of [1, 150, 100, 1.0] \n",
        "            # Weighted cross entropy: approach adapts following code: https://stackoverflow.com/questions/44560549/unbalanced-data-and-weighted-cross-entropy\n",
        "            ce_loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=conv_out, labels=labels_one_hot)\n",
        "            if do_weight:\n",
        "                weighted_loss = tf.reshape(tf.constant(loss_weights), [1, 1, 1, 1, num_classes]) # Format to the right size\n",
        "                weighted_one_hot = tf.reduce_sum(weighted_loss*labels_one_hot, axis = -1)\n",
        "                ce_loss = ce_loss * weighted_one_hot\n",
        "            self.loss = tf.reduce_mean(ce_loss) # Get loss\n",
        "            \n",
        "            self.trainer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "            \n",
        "            self.extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) # Ensure correct ordering for batch-norm to work\n",
        "            with tf.control_dependencies(self.extra_update_ops):\n",
        "                self.train_op = self.trainer.minimize(self.loss)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}